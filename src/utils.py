import time
from functools import wraps

import torch
import torch.nn.functional as F


def timer(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.perf_counter()
        result = func(*args, **kwargs)
        end_time = time.perf_counter()
        run_time = end_time - start_time

        print(
            f"Finished {func.__name__!r} in {run_time:.4f} secs with {result.shape[1] / run_time} tokens/sec"
        )
        return result

    return wrapper


# adapted from: https://gist.github.com/bsantraigi/5752667525d88d375207f099bd78818b
def top_k_p_filter(logits: torch.Tensor, top_k: int, top_p: float) -> torch.Tensor:
    top_k = min(top_k, logits.size(-1))
    if top_k > 0:
        # # remove all tokens with probability less than the last token of top-k
        # indices_to_remove = logits < torch.topk(logits, top_k, dim=-1)[0][..., -1, None]
        # # set those filtered tokens to -inf
        # logits[indices_to_remove] = float("-inf")
        values, _ = torch.topk(logits, top_k)
        min_values = values[..., -1, None].expand_as(logits)
        logits = torch.where(
            logits < min_values, torch.full_like(logits, float("-inf")), logits
        )
    if top_p > 0:
        sorted_logits, sorted_indices = torch.sort(logits, descending=True)
        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
        # determine indices to remove
        sorted_indices_to_remove = cumulative_probs > top_p
        # shift indices to the right to keep the same relative order
        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
        # ensure we always keep top probability token even if it exceed cumulative probability
        sorted_indices_to_remove[..., 0] = 0
        # # scatter sorted indices to remove to original indices
        # indices_to_remove = sorted_indices_to_remove.scatter(
        #     dim=-1, index=sorted_indices, src=sorted_indices_to_remove
        # )
        indices_to_remove = torch.zeros_like(logits, dtype=torch.bool).scatter_(
            dim=-1, index=sorted_indices, src=sorted_indices_to_remove
        )
        # # set logits of the indices to remove to -inf
        # logits[indices_to_remove] = float("-inf")
        logits = logits.masked_fill(indices_to_remove, float("-inf"))
    return logits


def norm_logits(
    logits: torch.Tensor, temperature: float, top_k: int, top_p: float, eps: float
) -> torch.Tensor:
    """
    Normalize logits by temperature and top-k/p filters.
    """
    # logits â€“ shape: [batch_size, vocab_size]
    assert logits.dim() == 2
    # temperature scaling: control "softness" or "sharpness" of probability distribution generated by softmax
    logits = logits / (temperature + eps)
    # [batch_size, vocab_size]
    logits = top_k_p_filter(logits, top_k, top_p)
    # generate probability distribution
    return F.softmax(logits, dim=-1)


def batch_norm_logits(
    logits: torch.Tensor, temperature: float, top_k: int, top_p: float, eps: float
) -> torch.Tensor:
    # assuming logits shape is [batch_size, sequence_length, vocab_size]
    batch_size, seq_len, vocab_size = logits.shape
    logits = logits.view(-1, vocab_size)
    probs = norm_logits(logits, temperature, top_k, top_p, eps)
    return probs.view(batch_size, seq_len, vocab_size)


def sample(probs: torch.Tensor, num_samples: int = 1) -> torch.Tensor:
    """Sample a token from the probability distribution."""
    return torch.multinomial(probs, num_samples=num_samples)


def max_fn(x):
    x_max = torch.where(x > 0, x, torch.zeros_like(x))
    x_max_sum = torch.sum(x_max, dim=1, keepdim=True)
    return x_max / x_max_sum
